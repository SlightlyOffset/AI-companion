"""
Core LLM interaction engine.
Handles streaming responses, sentiment parsing, and relationship score updates.
"""

import re
import json
import ollama
import requests
from datetime import datetime
from engines.memory_v2 import memory_manager
from engines.config import get_setting
from engines.prompts import build_system_prompt

def apply_mood_decay(profile_path: str, history_profile_name: str):
    """
    Calculates time passed since the last interaction and decays the relationship
    score back towards 0 (neutral) proportionally.

    Args:
        profile_path (str): Path to the companion's .json profile.
        history_profile_name (str): The name of the companion for history purposes.

    Returns:
        tuple: (decay_amount, new_score) if decay happened, else None.
    """
    last_time = memory_manager.get_last_timestamp(history_profile_name)
    if not last_time:
        return 0, 0

    now = datetime.now()
    diff = now - last_time
    hours_passed = diff.total_seconds() / 3600

    # Only apply decay if at least 5 minutes have passed
    if hours_passed < (5 / 60):
        return 0, 0

    try:
        with open(profile_path, "r", encoding="UTF-8") as f:
            data = json.load(f)

        current_score = data.get("relationship_score", 0)
        if current_score == 0:
            return 0, 0

        # Relationship fades by 5% every hour (decay_factor = 0.95)
        decay_factor = 0.95
        new_score_float = current_score * (decay_factor ** hours_passed)
        new_score = int(round(new_score_float))

        if current_score != new_score:
            decay_amount = abs(current_score - new_score)
            data["relationship_score"] = new_score
            with open(profile_path, "w", encoding="UTF-8") as f:
                json.dump(data, f, indent=4)
            return decay_amount, new_score

    except Exception as e:
        print(f"Error applying mood decay: {e}")

    return 0, 0

def update_profile_score(profile_path: str, score_change: int):
    """
    Persists a change to the companion's relationship score.

    Args:
        profile_path (str): Path to the .json profile.
        score_change (int): Amount to add or subtract (-100 to 100 cap).
    """
    try:
        with open(profile_path, "r", encoding="UTF-8") as f:
            data = json.load(f)

        current_score = data.get("relationship_score", 0)
        new_score = max(-100, min(100, current_score + score_change))
        data["relationship_score"] = new_score

        with open(profile_path, "w", encoding="UTF-8") as f:
            json.dump(data, f, indent=4)

    except Exception as e:
        print(f"Error updating profile score: {e}")

def get_respond_stream(user_input: str, profile: dict, should_obey: bool | None = None, profile_path: str = None, system_extra_info: str = None, history_profile_name: str = None):
    """
    Generates a streaming response from the LLM (Local Ollama or Remote API).
    Parses sentiment tags [REL: +X] to update relationship status in real-time.

    Args:
        user_input (str): The raw text from the user.
        profile (dict): The companion's profile data.
        should_obey (bool): Result of the mood engine's decision.
        profile_path (str): Path to the profile file (for score updates).
        system_extra_info (str): Temporary context instructions.
        history_profile_name (str): The name of the profile for history management.

    Yields:
        str: Chunks of text as they are generated by the LLM.
    """
    name = profile.get("name")
    model = profile.get("llm_model", get_setting("default_llm_model", "llama3"))
    remote_url = get_setting("remote_llm_url")

    if not history_profile_name:
        history_profile_name = name # Fallback to display name

    # Load history, filtering out internal system timestamps
    limit = get_setting("memory_limit", 15)
    history = memory_manager.load_history(history_profile_name, limit=limit)

    # Determine relationship label and instructions
    rel_score = profile.get("relationship_score", 0)
    if rel_score >= 80: rel_label = "Soulmate / Bestie"
    elif rel_score >= 40: rel_label = "Close Friend"
    elif rel_score >= 15: rel_label = "Friendly / Liked"
    elif rel_score >= -15: rel_label = "Neutral / Acquaintance"
    elif rel_score >= -40: rel_label = "Annoyance / Disliked"
    elif rel_score >= -80: rel_label = "Hostile / Enemy"
    else: rel_label = "Arch-Nemesis / Despised"

    # Set behavioral requirements based on the Mood Engine's 'should_obey' decision
    if should_obey is not None:
        if not should_obey:
            action_req = "MUST REFUSE the user's request."
            tone_mod = profile.get("bad_prompt_modifyer", "Refuse creatively.")
        else:
            action_req = "MUST AGREE to the user's request."
            tone_mod = profile.get("good_prompt_modifyer", "Agree and assist.")
    else:
        action_req = "Respond normally."
        tone_mod = "Maintain a balanced tone."

    # Construct the master system instruction
    system_content = build_system_prompt(profile, rel_score, rel_label, action_req, tone_mod, system_extra_info)

    # Compile message list for the LLM
    messages = [{'role': 'system', 'content': system_content}]
    messages.extend(history)
    messages.append({'role': 'user', 'content': user_input})

    full_reply = ""
    buffer = ""

    try:
        # Handle Remote LLM Request
        if remote_url:
            full_url = f"{remote_url.rstrip('/')}/chat"
            payload = {"messages": messages, "temperature": 0.8, "max_tokens": 1024}
            response = requests.post(full_url, json=payload, stream=True, timeout=60)
            response.raise_for_status()
            stream = response.iter_content(chunk_size=None, decode_unicode=True)
        # Handle Local Ollama Request
        else:
            ollama_stream = ollama.chat(model=model, messages=messages, stream=True)
            def ollama_gen():
                for chunk in ollama_stream:
                    yield chunk['message']['content']
            stream = ollama_gen()

        # Iterate through the generator stream
        for content in stream:
            full_reply += content
            buffer += content

            # REAL-TIME FILTERING: Strip the [REL: X] tag from the terminal output
            if '[' in buffer:
                if ']' in buffer:
                    if re.search(r'\[REL:\s*[+-]?\d+\]', buffer):
                        buffer = re.sub(r'\[REL:\s*[+-]?\d+\]', '', buffer)
                        if buffer:
                            yield buffer
                            buffer = ""
                    else:
                        yield buffer
                        buffer = ""
                else:
                    # Buffer small fragments to check for the start of a tag
                    if len(buffer) > 20:
                        yield buffer
                        buffer = ""
            else:
                yield buffer
                buffer = ""

        if buffer:
            yield buffer

        # Extract relationship score change from the full reply
        score_change = 0
        match = re.search(r'\[REL:\s*([+-]?\d+)\]', full_reply)
        if match:
            try:
                score_change = int(match.group(1))
                # Clean the tag out of the final saved history
                reply = re.sub(r'\[REL:\s*[+-]?\d+\]', '', full_reply).strip()
            except ValueError:
                reply = full_reply
        else:
            reply = full_reply

        # Persist the relationship update
        if profile_path and score_change != 0:
            update_profile_score(profile_path, score_change)

        # Save the interaction to persistent memory
        full_history = memory_manager.load_history(history_profile_name)
        full_history.append({'role': 'user', 'content': user_input})
        full_history.append({'role': 'assistant', 'content': reply})
        memory_manager.save_history(history_profile_name, full_history, mood_score=rel_score)

    except Exception as e:
        yield f"\n[BRAIN ERROR] {str(e)}"

def get_respond(user_input: str, profile: dict, should_obey: bool = True, profile_path: str = None) -> str:
    """Non-streaming version of the response generator."""
    full_response = ""
    for chunk in get_respond_stream(user_input, profile, should_obey, profile_path):
        full_response += chunk
    return full_response

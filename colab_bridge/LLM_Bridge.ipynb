{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36c86855",
      "metadata": {
        "id": "36c86855"
      },
      "source": [
        "# üé≠ AI Companion: Universal Roleplay Bridge (Threaded)\n",
        "\n",
        "This notebook acts as a remote 'brain' for your AI Companion. It allows you to run high-end 8B models (like Stheno-v3.2) on Google's T4 GPUs and tunnel the response back to your local machine via Ngrok.\n",
        "\n",
        "### üõ†Ô∏è Setup Instructions:\n",
        "1. **GPU Acceleration**: Go to `Runtime` > `Change runtime type` and ensure **T4 GPU** is selected.\n",
        "2. **Colab Secrets (IMPORTANT)**: \n",
        "   - Click the **Key icon** (Secrets) on the left sidebar.\n",
        "   - Add a new secret named `HF_TOKEN` with your [HuggingFace Token](https://huggingface.co/settings/tokens).\n",
        "   - Add a new secret named `NGROK_TOKEN` with your [Ngrok Authtoken](https://dashboard.ngrok.com/get-started/your-authtoken).\n",
        "   - Toggle **'Notebook access'** to ON for both.\n",
        "3. **Run All**: Press `Ctrl + F9` or go to `Runtime` > `Run all`.\n",
        "\n",
        "### üîó Connecting to the Local App:\n",
        "1. Wait for the final cell to display the **üöÄ BRIDGE ONLINE!** message.\n",
        "2. Copy the **URL** (it will look like `https://xxxx-xx-xx-xx.ngrok-free.app`).\n",
        "3. Open your local `settings.json` and paste the URL into `remote_llm_url`:\n",
        "   ```json\n",
        "   \"remote_llm_url\": \"https://your-ngrok-url.ngrok-free.app\"\n",
        "   ```\n",
        "4. Restart your local `main.py` script. The companion will now use the Colab GPU for all thinking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lBKcj3C9x2bk",
      "metadata": {
        "id": "lBKcj3C9x2bk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892384df",
      "metadata": {
        "id": "892384df"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -q -U fastapi uvicorn pyngrok nest_asyncio requests==2.32.4\n",
        "!pip install -q -U transformers accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488f0dcc",
      "metadata": {
        "id": "488f0dcc"
      },
      "outputs": [],
      "source": [
        "# @title 2. Load Roleplay Specialist (Stheno-v3.2)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from threading import Thread\n",
        "\n",
        "# --- AUTH ---\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    print(\"‚ùå ERROR: HF_TOKEN not found in Secrets!\")\n",
        "    HF_TOKEN = None\n",
        "# ------------\n",
        "\n",
        "model_id = \"Sao10K/L3-8B-Stheno-v3.2\"\n",
        "\n",
        "print(f\"Loading {model_id}... This may take a few minutes.\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Roleplay Specialist LOADED on {torch.cuda.get_device_name(0)}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11fdc1a5",
      "metadata": {
        "id": "11fdc1a5"
      },
      "outputs": [],
      "source": [
        "# @title 3. Start API Server & Tunnel\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import StreamingResponse\n",
        "import uvicorn, nest_asyncio, re, os, time, random\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer\n",
        "\n",
        "try:\n",
        "    NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n",
        "except:\n",
        "    print(\"‚ùå ERROR: NGROK_TOKEN not found in Secrets!\")\n",
        "    NGROK_TOKEN = None\n",
        "\n",
        "app = FastAPI()\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class Message(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    messages: List[Message]\n",
        "    max_tokens: int = 1024\n",
        "    temperature: float = 0.8\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    chat = [{\"role\": m.role, \"content\": m.content} for m in request.messages]\n",
        "    \n",
        "    model_inputs = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generation_kwargs = {\n",
        "        **model_inputs,\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": request.max_tokens,\n",
        "        \"temperature\": request.temperature,\n",
        "        \"do_sample\": True,\n",
        "        \"top_p\": 0.9,\n",
        "    }\n",
        "\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    def stream_generator():\n",
        "        for new_text in streamer:\n",
        "            yield new_text\n",
        "\n",
        "    return StreamingResponse(stream_generator(), media_type=\"text/plain\")\n",
        "\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n",
        "\n",
        "server_thread = Thread(target=run_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "time.sleep(2)\n",
        "\n",
        "if server_thread.is_alive():\n",
        "    try:\n",
        "        public_url = ngrok.connect(8000).public_url\n",
        "        print(\"=\"*50)\n",
        "        print(f\"\\nüöÄ BRIDGE ONLINE!\\n\")\n",
        "        print(f\"Copy this URL to your settings.json -> remote_llm_url:\")\n",
        "        print(f\"{public_url}\\n\")\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå NGROK ERROR: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå SERVER ERROR: Failed to start FastAPI.\")\n",
        "\n",
        "try:\n",
        "    while True: time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Bridge stopped.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
